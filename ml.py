# -*- coding: utf-8 -*-
"""ML.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Y7s3bFFfvLG0YBXoUdwT2WI_9_86EajF
"""

import pandas as pd
import itertools
from statsmodels.tsa.arima.model import ARIMA
from xgboost import XGBRegressor
import numpy as np
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import matplotlib.pyplot as plt

df = pd.read_csv('/content/product_demand_prediction_dataset (1).csv')
df.head()

df['Date'] = pd.to_datetime(df['Date'])
df['Year'] = df['Date'].dt.year
df['Month'] = df['Date'].dt.month
df['Day'] = df['Date'].dt.day
df['DayOfYear'] = df['Date'].dt.dayofyear
df.drop(columns=['ProductID', 'StoreID', 'Date'], inplace=True)
df.head()

def remove_outliers(df, columns):
      for column in columns:
          Q1 = df[column].quantile(0.25)
          Q3 = df[column].quantile(0.75)
          IQR = Q3 - Q1
          df = df[~((df[column] < (Q1 - 1.5 * IQR)) | (df[column] > (Q3 + 1.5 * IQR)))]
      return df

df=remove_outliers(df,['Sales','Price','CompetitorPrice','EconomicIndicator','StockLevel'])

df.head()

df.isnull().sum()

unique_values = {col: df[col].unique() for col in df.columns}

# Optionally, you can print the unique values for each column
for col, values in unique_values.items():
    print(f"Unique values in '{col}': {values}")

df.describe()

df.info()

df.head()

le=LabelEncoder()
df.Promotion=le.fit_transform(df.Promotion)
df.Holiday=le.fit_transform(df.Holiday)
df.Season=le.fit_transform(df.Season)
df.DayOfWeek=le.fit_transform(df.DayOfWeek)
df.Weather=le.fit_transform(df.Weather)
df.head()

# Standardize the continuous features (Sales, Price, CompetitorPrice, EconomicIndicator, StockLevel)
scaler = StandardScaler()
df[['Sales', 'Price', 'CompetitorPrice', 'StockLevel','EconomicIndicator']] = scaler.fit_transform(df[['Sales', 'Price', 'CompetitorPrice', 'StockLevel','EconomicIndicator']])

X = df.drop(['Demand','Month'], axis=1)
y = df['Demand']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

### 1. Linear Regression ###
lr_model = LinearRegression()
lr_model.fit(X_train, y_train)

# Predict on the test set
y_pred_lr = lr_model.predict(X_test)

# Evaluate the Linear Regression model
mae_lr = mean_absolute_error(y_test, y_pred_lr)
mse_lr = mean_squared_error(y_test, y_pred_lr)
r2_lr = r2_score(y_test, y_pred_lr)

print(f"Linear Regression - MAE: {mae_lr}, MSE: {mse_lr}, R2: {r2_lr}")

lr_model.get_params()

from sklearn.model_selection import GridSearchCV
param_space = {'copy_X': [True,False], 'fit_intercept': [True,False],
               'n_jobs': [1,5,10,15,None], 'positive': [True,False]}
grid_search = GridSearchCV(lr_model, param_space, cv=5)

grid_search.fit(X_train, y_train)

# Parameter which gives the best results
print(f"Best Hyperparameters: {grid_search.best_params_}")

# Accuracy of the model after using best parameters
print(f"Best Score: {grid_search.best_score_}")

# Get the feature names after encoding and scaling
feature_names = X_train.columns

# Get the coefficients (feature importance) from the Linear Regression model
coefficients = lr_model.coef_

# Create a DataFrame for better visualization
feature_importance = pd.DataFrame({
    'Feature': feature_names,
    'Importance': coefficients
})

# Sort the features by importance (absolute value)
feature_importance['Absolute Importance'] = np.abs(feature_importance['Importance'])
feature_importance = feature_importance.sort_values(by='Absolute Importance', ascending=False)

# Display the feature importance
print(feature_importance[['Feature', 'Importance']])

# Plot the feature importance
plt.figure(figsize=(10, 6))
plt.barh(feature_importance['Feature'], feature_importance['Importance'])
plt.xlabel('Importance')
plt.title('Feature Importance for Linear Regression')
plt.show()

plt.figure(figsize=(10, 6))

plt.plot(y_test.values, label='Actual Demand', color='blue', linestyle='-', marker='o')
plt.plot(y_pred_lr, label='Predicted Demand', color='red', linestyle='-', marker='x')

plt.title('Actual vs Predicted Demand (Linear Regression)')
plt.xlabel('Samples')
plt.ylabel('Demand')
plt.legend()
plt.show()

plt.figure(figsize=(10, 6))

plt.scatter(y_test, y_pred_lr, color='blue', alpha=0.5)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)

plt.title('Scatter Plot: Actual vs Predicted Demand (Linear Regression)')
plt.xlabel('Actual Demand')
plt.ylabel('Predicted Demand')
plt.show()

from sklearn.linear_model import Ridge

# Initialize Ridge regression model with alpha (regularization strength)
ridge_model = Ridge(alpha=1.0)  # You can tune the alpha value
ridge_model.fit(X_train, y_train)

# Predict on test data
y_pred_ridge = ridge_model.predict(X_test)

# Evaluate Ridge Regression model
mae_ridge = mean_absolute_error(y_test, y_pred_ridge)
mse_ridge = mean_squared_error(y_test, y_pred_ridge)
r2_ridge = r2_score(y_test, y_pred_ridge)

print(f"Ridge Regression - MAE: {mae_ridge}, MSE: {mse_ridge}, R2: {r2_ridge}")

# Extract the coefficients from the Ridge model
feature_importance_ridge = pd.DataFrame({
    'Feature': X_train.columns,
    'Importance': ridge_model.coef_
})

# Sort by absolute importance
feature_importance_ridge = feature_importance_ridge.reindex(feature_importance_ridge['Importance'].abs().sort_values(ascending=False).index)

# Print feature importance
print(feature_importance_ridge)

# Plot feature importance
plt.figure(figsize=(10, 6))
plt.barh(feature_importance_ridge['Feature'], feature_importance_ridge['Importance'])
plt.xlabel('Coefficient Value')
plt.title('Feature Importance (Ridge Regression)')
plt.show()

plt.figure(figsize=(10, 6))

plt.plot(y_test.values, label='Actual Demand', color='blue', linestyle='-', marker='o')
plt.plot(y_pred_ridge, label='Predicted Demand (Ridge)', color='red', linestyle='-', marker='x')

plt.title('Actual vs Predicted Demand (Ridge Regression)')
plt.xlabel('Samples')
plt.ylabel('Demand')
plt.legend()
plt.show()

plt.figure(figsize=(10, 6))

plt.scatter(y_test, y_pred_ridge, color='blue', alpha=0.5)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)

plt.title('Scatter Plot: Actual vs Predicted Demand (Ridge Regression)')
plt.xlabel('Actual Demand')
plt.ylabel('Predicted Demand')
plt.show()